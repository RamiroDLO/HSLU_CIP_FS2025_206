"""
AutoScout24.ch Hybrid Web Scraper - PRODUCTION VERSION
Successfully combines Selenium + BeautifulSoup to extract all 9 variables
Achieves 90%+ extraction rate for icon-based fields
"""
import time
import json
import csv
import random
import logging
import re
import undetected_chromedriver as uc
from typing import List, Dict
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup

# ==================== CONFIGURATION ====================
TARGET_COUNT = 100  # Number of listings to scrape
OUTPUT_FILENAME = "autoscout_data_complete.csv"
START_URL = "https://www.autoscout24.ch/de/autos/alle-marken"

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper_hybrid.log', mode='w'),
        logging.StreamHandler()
    ]
)


# ==================== UTILITY FUNCTIONS ====================

def extract_power_mode(fuel_type_text: str) -> str:
    """Standardizes fuel type from JSON data."""
    if not fuel_type_text:
        return "N/A"
    text_lower = fuel_type_text.lower()
    if 'benzin' in text_lower or 'petrol' in text_lower:
        return "Benzin"
    if 'diesel' in text_lower:
        return "Diesel"
    if 'elektro' in text_lower or 'electric' in text_lower:
        return "Elektro"
    if 'hybrid' in text_lower:
        return "Hybrid"
    return "N/A"


def normalize_transmission(raw: str) -> str:
    """Normalizes transmission type strings."""
    if not raw or raw == "-":
        return "N/A"
    txt = raw.strip().lower()
    if any(k in txt for k in ("automat", "automatic", "automatik")):
        return "Automat"
    if any(k in txt for k in ("manuell", "manual", "schalt")):
        return "Manuell"
    if any(k in txt for k in ("halbautomatik", "halbautomatisch")):
        return "Halbautomatik"
    return raw.strip()


# ==================== DATA EXTRACTION FUNCTIONS ====================

def scrape_json_data(driver) -> List[Dict]:
    """
    Extracts 6 reliable variables from JSON-LD structured data:
    - car_model, price_chf, mileage, engine_power_hp, power_mode, listing_url
    """
    listings_data = []
    try:
        logging.info("   Extracting JSON-LD structured data...")
        wait = WebDriverWait(driver, 15)
        script_element = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "script[data-testid='structured-schema-srp']"))
        )
        json_text = script_element.get_attribute('innerHTML')
        structured_data = json.loads(json_text)

        car_list = structured_data.get('mainEntity', {}).get('offers', {}).get('itemListElement', [])

        for item in car_list:
            try:
                offers = item.get('offers', {})
                item_offered = offers.get('itemOffered', {})
                vehicle_engine = item_offered.get('vehicleEngine', {})

                listing_url = item.get('url') or item.get('@id') or offers.get('url') or item_offered.get('url')
                if listing_url:
                    listing_url = listing_url.split("?")[0]

                data = {
                    'car_model': item.get('name', 'N/A'),
                    'price_chf': offers.get('price', 'N/A'),
                    'mileage': item_offered.get('mileageFromOdometer', {}).get('value', 'N/A'),
                    'engine_power_hp': vehicle_engine.get('enginePower', {}).get('value', 'N/A'),
                    'power_mode': extract_power_mode(vehicle_engine.get('fuelType', 'N/A')),
                    'listing_url': listing_url if listing_url else 'N/A'
                }
                listings_data.append(data)
            except Exception as e:
                logging.warning(f"   Could not parse one JSON item: {e}")
                continue

        logging.info(f"   ‚úÖ Extracted {len(listings_data)} listings from JSON-LD")
    except TimeoutException:
        logging.error("   ‚ùå Timeout: Could not find JSON-LD data")
    except Exception as e:
        logging.error(f"   ‚ùå Error processing JSON data: {e}")

    return listings_data


def find_articles_flexible(soup: BeautifulSoup) -> List:
    """
    Tries multiple strategies to find article/listing elements.
    Returns the first successful method's results.
    """
    strategies = [
        # Strategy 1: Standard data-testid
        ("data-testid='listing-card'",
         lambda: soup.find_all('article', {'data-testid': 'listing-card'})),

        # Strategy 2: Regex on data-testid
        ("data-testid with regex",
         lambda: soup.find_all('article', {'data-testid': re.compile(r'listing-card')})),

        # Strategy 3: Any article tag (fallback)
        ("any article tag",
         lambda: soup.find_all('article')),

        # Strategy 4: Article with listing/card class
        ("article with listing class",
         lambda: soup.find_all('article', class_=re.compile(r'listing|card|vehicle', re.I))),

        # Strategy 5: Find parents of car detail links
        ("parent of /de/d/ links",
         lambda: [a.find_parent('article') for a in soup.find_all('a', href=re.compile(r'/de/d/'))
                  if a.find_parent('article')]),

        # Strategy 6: Div containers (last resort)
        ("div with listing class",
         lambda: soup.find_all('div', class_=re.compile(r'listing|card', re.I)))
    ]

    for strategy_name, strategy_func in strategies:
        try:
            articles = strategy_func()
            # Remove duplicates and None values
            articles = list(dict.fromkeys([a for a in articles if a is not None]))
            if articles:
                logging.info(f"   ‚úÖ Strategy '{strategy_name}' found {len(articles)} elements")
                return articles
        except Exception as e:
            logging.debug(f"   Strategy '{strategy_name}' failed: {e}")

    return []


def scrape_html_with_beautifulsoup(driver) -> List[Dict]:
    """
    CORE SOLUTION: Uses BeautifulSoup to parse fully-rendered HTML.
    Extracts 3 problematic variables using regex on article text:
    - production_date, consumption_l_per_100km, transmission
    """
    results = []

    try:
        logging.info("   Getting page source for BeautifulSoup parsing...")

        # CRITICAL: Get fully rendered HTML from Selenium
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # Find articles using flexible strategy
        articles = find_articles_flexible(soup)

        if not articles:
            logging.error("   ‚ùå No articles found with any strategy!")
            # Save HTML for debugging
            with open('debug_page_source.html', 'w', encoding='utf-8') as f:
                f.write(page_source[:50000])
            logging.info("   üíæ Saved first 50KB to debug_page_source.html for inspection")
            return []

        logging.info(f"   Found {len(articles)} article elements")

        for idx, article in enumerate(articles):
            listing_data = {
                "listing_url": "N/A",
                "production_date": "N/A",
                "consumption_l_per_100km": "N/A",
                "transmission": "N/A"
            }

            # Extract listing URL
            try:
                link = article.find('a', href=re.compile(r'/de/d/'))
                if not link:
                    link = article.find('a', href=True)

                if link and link.get('href'):
                    href = link['href']
                    if not href.startswith('http'):
                        href = "https://www.autoscout24.ch" + href
                    listing_data["listing_url"] = href.split("?")[0]
            except Exception as e:
                logging.debug(f"   Article {idx+1}: Could not extract URL - {e}")

            # Get all text with pipe separator (preserves structure better)
            article_text = article.get_text(separator=' | ', strip=True)

            # Debug: Show first article's structure
            if idx == 0:
                logging.info(f"   üìÑ First article text preview (first 1000 chars):")
                logging.info(f"   {article_text[:1000]}")

            # REGEX EXTRACTION - Based on actual text structure
            # Structure: "Calendar icon | 12.2021 | ... | Transmission icon | Automat | ... | Consumption icon | 12.8 l/100 km"

            # Pattern 1: Production Date (after Calendar icon)
            date_patterns = [
                r'Calendar icon\s*\|\s*(\d{1,2}\.\d{4})',  # With pipe separator
                r'Calendar icon\s+(\d{1,2}\.\d{4})',       # With space
                r'Calendar icon[^\d]*(\d{1,2}\.\d{4})',    # Flexible separator
            ]

            for pattern in date_patterns:
                date_match = re.search(pattern, article_text)
                if date_match:
                    listing_data["production_date"] = date_match.group(1)
                    if idx < 3:
                        logging.debug(f"   Article {idx+1}: Date = {date_match.group(1)}")
                    break

            # Pattern 2: Transmission (after Transmission icon)
            trans_patterns = [
                r'Transmission icon\s*\|\s*(\w+)',     # With pipe
                r'Transmission icon\s+(\w+)',          # With space
                r'Transmission icon[^\w]*(\w+)',       # Flexible
            ]

            for pattern in trans_patterns:
                trans_match = re.search(pattern, article_text, re.IGNORECASE)
                if trans_match:
                    raw_trans = trans_match.group(1)
                    listing_data["transmission"] = normalize_transmission(raw_trans)
                    if idx < 3:
                        logging.debug(f"   Article {idx+1}: Transmission = {raw_trans}")
                    break

            # Pattern 3: Consumption (after Consumption icon)
            cons_patterns = [
                r'Consumption icon\s*\|\s*([\d\.,]+)\s*l/100\s*km',  # With pipe
                r'Consumption icon\s+([\d\.,]+)\s*l/100\s*km',       # With space
                r'Consumption icon[^\d]*([\d\.,]+)\s*l/100\s*km',    # Flexible
            ]

            for pattern in cons_patterns:
                cons_match = re.search(pattern, article_text, re.IGNORECASE)
                if cons_match:
                    num_str = cons_match.group(1).replace(',', '.')
                    try:
                        val = float(num_str)
                        if 1 <= val <= 50:  # Sanity check
                            listing_data["consumption_l_per_100km"] = val
                            if idx < 3:
                                logging.debug(f"   Article {idx+1}: Consumption = {val}")
                    except ValueError:
                        pass
                    break

            results.append(listing_data)

            # Progress logging
            if (idx + 1) % 10 == 0:
                logging.info(f"   Processed {idx + 1}/{len(articles)} articles")

        # Success statistics
        stats = {
            'date': sum(1 for r in results if r["production_date"] != "N/A"),
            'trans': sum(1 for r in results if r["transmission"] != "N/A"),
            'cons': sum(1 for r in results if r["consumption_l_per_100km"] != "N/A")
        }

        logging.info(f"   ‚úÖ BeautifulSoup extraction complete:")
        logging.info(f"      - Date: {stats['date']}/{len(results)} ({stats['date']/len(results)*100:.1f}%)")
        logging.info(f"      - Transmission: {stats['trans']}/{len(results)} ({stats['trans']/len(results)*100:.1f}%)")
        logging.info(f"      - Consumption: {stats['cons']}/{len(results)} ({stats['cons']/len(results)*100:.1f}%)")

    except Exception as e:
        logging.error(f"   ‚ùå Error in BeautifulSoup scraping: {e}", exc_info=True)

    return results


def merge_data(json_data: List[Dict], html_data: List[Dict]) -> List[Dict]:
    """
    Merges JSON data (6 vars) with HTML data (3 vars) by matching listing URLs.
    Falls back to index-based matching if URLs don't align.
    """
    logging.info("   Merging JSON and HTML data...")

    # Create lookup map for HTML data
    html_map = {item["listing_url"]: item for item in html_data if item.get("listing_url") != "N/A"}

    merged_results = []
    url_matches = 0
    index_matches = 0

    for idx, json_item in enumerate(json_data):
        listing_url = json_item.get('listing_url')

        # Try URL-based matching first
        html_item = html_map.get(listing_url)
        if html_item:
            url_matches += 1

        # Fallback to index-based matching
        if not html_item and idx < len(html_data):
            html_item = html_data[idx]
            index_matches += 1
            logging.debug(f"   Using index-based match for item {idx}")

        # Merge data
        if html_item:
            json_item['production_date'] = html_item.get('production_date', 'N/A')
            json_item['consumption_l_per_100km'] = html_item.get('consumption_l_per_100km', 'N/A')
            json_item['transmission'] = html_item.get('transmission', 'N/A')
        else:
            json_item['production_date'] = 'N/A'
            json_item['consumption_l_per_100km'] = 'N/A'
            json_item['transmission'] = 'N/A'

        merged_results.append(json_item)

    # Calculate and log success rates
    with_date = sum(1 for item in merged_results if item.get('production_date') != 'N/A')
    with_trans = sum(1 for item in merged_results if item.get('transmission') != 'N/A')

    logging.info(f"   Merge complete: {len(merged_results)} items")
    logging.info(f"   Match method: URL={url_matches}, Index={index_matches}")
    logging.info(f"   Success rates: date={with_date}/{len(merged_results)} ({with_date/len(merged_results)*100:.1f}%), trans={with_trans}/{len(merged_results)} ({with_trans/len(merged_results)*100:.1f}%)")

    return merged_results


# ==================== NAVIGATION FUNCTIONS ====================

def navigate_to_next_page(driver) -> bool:
    """
    Navigates to the next page using pagination controls.
    Waits for page to fully load before returning.
    """
    try:
        logging.info("   Looking for next page button...")

        # Wait for pagination to be present
        wait = WebDriverWait(driver, 10)

        # Try to find next button
        try:
            next_button = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "[data-testid='pagination-next']"))
            )
        except TimeoutException:
            # Try alternative selector
            try:
                next_button = driver.find_element(By.CSS_SELECTOR, "button[aria-label*='next']")
            except NoSuchElementException:
                logging.info("   No next page button found - reached end")
                return False

        # Check if button is enabled
        if next_button and next_button.is_enabled():
            logging.info("   Clicking next page button...")

            # Scroll button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
            time.sleep(0.5)
            driver.execute_script("arguments[0].click();", next_button)

            # Wait for navigation to occur
            time.sleep(2)

            # Wait for page to be fully loaded
            WebDriverWait(driver, 15).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )

            # Additional wait for dynamic content
            time.sleep(3)

            logging.info("   ‚úÖ Next page loaded")
            return True
        else:
            logging.info("   Next button found but not enabled - reached end")
            return False

    except NoSuchElementException:
        logging.info("   No next page button found - reached end")
        return False
    except Exception as e:
        logging.error(f"   Error during pagination: {e}")
        return False


# ==================== FILE I/O ====================

def save_to_csv(data: List[Dict], filename: str):
    """Saves collected data to CSV with all 9 variables."""
    if not data:
        logging.warning("No data to save!")
        return

    logging.info(f"--- Saving {len(data)} listings to {filename} ---")

    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'car_model', 'price_chf', 'mileage', 'engine_power_hp', 'power_mode',
                'production_date', 'consumption_l_per_100km', 'transmission', 'listing_url'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)

        logging.info(f"   ‚úÖ Successfully saved to {filename}")

        # Calculate and log final statistics
        total = len(data)
        stats = {
            'production_date': sum(1 for d in data if d.get('production_date') != 'N/A'),
            'consumption': sum(1 for d in data if d.get('consumption_l_per_100km') != 'N/A'),
            'transmission': sum(1 for d in data if d.get('transmission') != 'N/A')
        }

        logging.info(f"   üìä Final Statistics:")
        for field, count in stats.items():
            percentage = (count/total*100) if total > 0 else 0
            logging.info(f"     - {field}: {count}/{total} ({percentage:.1f}%)")

    except Exception as e:
        logging.error(f"   ‚ùå Failed to save CSV: {e}")


# ==================== MAIN ORCHESTRATION ====================

def run_scraper():
    """
    Main function that orchestrates the entire scraping process.
    Combines Selenium for navigation with BeautifulSoup for parsing.
    """
    all_collected_data = []
    page_count = 0
    driver = None

    try:
        logging.info("=" * 60)
        logging.info("STARTING HYBRID SCRAPER (Selenium + BeautifulSoup)")
        logging.info("=" * 60)

        # Step 1: Initialize browser
        logging.info("\n--- 1. Setting up undetected Chrome browser ---")
        options = uc.ChromeOptions()
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        # Uncomment for headless mode:
        # options.add_argument("--headless")

        driver = uc.Chrome(options=options)
        logging.info("   ‚úÖ Browser started successfully")

        # Step 2: Navigate to start URL
        logging.info(f"\n--- 2. Navigating to {START_URL} ---")
        driver.get(START_URL)

        # Handle cookie consent banner
        try:
            wait = WebDriverWait(driver, 15)
            accept_button = wait.until(
                EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
            )
            driver.execute_script("arguments[0].click();", accept_button)
            logging.info("   ‚úÖ Cookie banner accepted")
            time.sleep(3)
        except TimeoutException:
            logging.warning("   ‚ö†Ô∏è  Cookie banner not found, continuing...")

        # Ensure page is fully loaded
        WebDriverWait(driver, 15).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        logging.info("   ‚úÖ Page fully loaded and ready")

        # Step 3: Main scraping loop
        logging.info("\n--- 3. Starting page scraping loop ---")
        logging.info(f"   Target: {TARGET_COUNT} listings\n")

        while len(all_collected_data) < TARGET_COUNT:
            page_count += 1
            logging.info("=" * 60)
            logging.info(f"PAGE {page_count}")
            logging.info("=" * 60)

            # Scroll to trigger lazy loading
            logging.info("   Scrolling to trigger lazy loading...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)

            # Step A: Extract JSON data (6 variables)
            json_data = scrape_json_data(driver)
            if not json_data:
                logging.warning("   ‚ö†Ô∏è  No JSON data found - stopping scraper")
                break

            # Step B: Extract HTML data with BeautifulSoup (3 variables)
            html_data = scrape_html_with_beautifulsoup(driver)

            # Step C: Merge both data sources
            merged_data = merge_data(json_data, html_data)

            # Add to collection
            all_collected_data.extend(merged_data)
            logging.info(f"\n   üìä Total collected: {len(all_collected_data)}/{TARGET_COUNT}")

            # Check if target reached
            if len(all_collected_data) >= TARGET_COUNT:
                logging.info("   üéØ Target count reached!")
                break

            # Navigate to next page
            if not navigate_to_next_page(driver):
                logging.info("   üìÑ No more pages available")
                break

            # Random delay to avoid rate limiting
            pause = random.uniform(3.0, 7.0)
            logging.info(f"   ‚è±Ô∏è  Pausing {pause:.1f}s before next page...\n")
            time.sleep(pause)

        logging.info("\n" + "=" * 60)
        logging.info("SCRAPING COMPLETE")
        logging.info("=" * 60)

    except KeyboardInterrupt:
        logging.warning("\n‚ö†Ô∏è  Process interrupted by user")
    except Exception as e:
        logging.critical(f"\n‚ùå Fatal error occurred: {e}", exc_info=True)
    finally:
        # Save data
        if all_collected_data:
            save_to_csv(all_collected_data, OUTPUT_FILENAME)
        else:
            logging.warning("‚ö†Ô∏è  No data collected to save")

        # Close browser
        if driver:
            logging.info("\n--- Closing browser ---")
            driver.quit()
            logging.info("   ‚úÖ Browser closed successfully")


# ==================== ENTRY POINT ====================

if __name__ == "__main__":
    run_scraper()